<div align="center">

# ğŸ•µï¸ LLM-CerebroScope

**Enterprise-Grade Forensic Data Analysis & Logic Engine**

*Transparent, Traceable, AI-Powered Document Intelligence*

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Code Style](https://img.shields.io/badge/Code%20Style-PEP%208-blue.svg)](https://pep8.org/)

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Quick Start](#-quick-start) â€¢ [Documentation](#-documentation) â€¢ [Contributing](#-contributing)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [System Requirements](#-system-requirements)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Usage Guide](#-usage-guide)
- [Architecture](#-architecture)
- [Troubleshooting](#-troubleshooting)
- [Use Cases](#-use-cases)
- [Contributing](#-contributing)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)

---

## ğŸ¯ Overview

**LLM-CerebroScope** is a sophisticated forensic data analysis platform that combines Large Language Models (LLMs) with advanced vector search technology. Designed for professionals who require transparent, auditable, and traceable document analysis, CerebroScope provides enterprise-grade capabilities for:

- **Document Intelligence**: Multi-format ingestion with intelligent chunking
- **Semantic Retrieval**: Advanced vector search using ChromaDB
- **LLM-Powered Analysis**: Integration with Ollama for local, privacy-preserving inference
- **Source Attribution**: Automatic citation tracking with chunk-level granularity
- **Conflict Detection**: AI-powered contradiction and inconsistency identification
- **Reliability Scoring**: Metadata-based source credibility assessment

### Why CerebroScope?

âœ… **Transparency**: Every answer includes traceable citations to source documents  
âœ… **Privacy**: Fully local processing with Ollama (no cloud dependencies)  
âœ… **Accuracy**: Reliability scoring and conflict detection ensure high-quality results  
âœ… **Flexibility**: Dual interfaces (CLI & Web GUI) for different workflows  
âœ… **Extensibility**: Modular architecture designed for customization

---

## âœ¨ Key Features

### ğŸ“„ Document Processing
- **Multi-Format Support**: PDF, CSV, TXT, XLSX/XLS
- **Intelligent Chunking**: Configurable size (800 chars) and overlap (100 chars)
- **Metadata Preservation**: Source, page numbers, timestamps, and file modification dates
- **Incremental Ingestion**: Automatic detection of new/modified files

### ğŸ” Semantic Search
- **Vector Database**: ChromaDB-powered persistent storage
- **Embedding Generation**: Automatic text embeddings using default models
- **Source Filtering**: Search within specific documents or collections
- **Top-K Retrieval**: Configurable result ranking (default: 5 chunks)

### ğŸ§  AI Analysis Engine
- **Model Agnostic**: Works with any Ollama-compatible LLM
- **Citation Tracking**: Automatic `[ID: xxxxxxxx]` format citations in responses
- **Context Awareness**: Prioritizes newer sources when conflicts arise
- **Prompt Engineering**: Optimized forensic analysis prompts

### âš ï¸ Validation & Quality Assurance
- **Conflict Detection**: LLM-powered contradiction identification
- **Reliability Scoring**: Heuristic algorithm considering:
  - File format (structured data preferred)
  - Document recency (time-based decay)
  - Metadata completeness
- **Evidence Heatmaps**: Visual highlighting of used vs. ignored chunks

### ğŸ“Š Visualization & Reporting
- **Interactive Graphs**: Knowledge graph visualization using spaCy NER
- **Entity Extraction**: Automatic identification of organizations, people, locations, dates, monetary values
- **Evidence Cards**: Color-coded display (green = used, gray = ignored)
- **Markdown Reports**: Comprehensive, timestamped analysis reports

### ğŸ’» Dual Interface
- **Rich CLI**: Beautiful terminal interface using Rich library
- **Streamlit GUI**: Modern web dashboard with drag-and-drop file upload
- **Feature Parity**: Both interfaces support all core functionality

---

## ğŸ’» System Requirements

- **Python**: 3.8+
- **RAM**: 4 GB minimum (8 GB recommended)
- **Storage**: 500 MB + space for documents and database
- **Ollama**: [Install from ollama.ai](https://ollama.ai/)
- **Optional**: spaCy model for graph visualization (`python -m spacy download en_core_web_sm`)

---

## ğŸš€ Installation

```bash
# 1. Clone repository
git clone https://github.com/yourusername/LLM-CerebroScope.git
cd LLM-CerebroScope

# 2. Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install ollama chromadb streamlit streamlit-agraph rich pandas pypdf spacy
python -m spacy download en_core_web_sm

# 4. Setup Ollama
ollama serve  # Keep running in separate terminal
ollama pull llama3  # Download a model

# 5. Initialize data directory
mkdir -p data/raw
```

---

## ğŸƒ Quick Start

1. **Add documents** to `data/raw/` directory
2. **Launch interface**:
   - CLI: `python -m cerebro.cli`
   - GUI: `streamlit run cerebro/gui.py` â†’ `http://localhost:8501`
3. **Query documents**: Ask questions in natural language
4. **Review results**: Check citations `[ID: xxxxxxxx]`, conflicts, and download reports

---

## ğŸ“– Usage Guide

### Command-Line Interface

```bash
python -m cerebro.cli
```

**Workflow**: Select model â†’ Auto-ingest documents â†’ Query in natural language â†’ View formatted results â†’ Type `exit` to quit

### Web Interface

```bash
streamlit run cerebro/gui.py
```

**Features**: Model selection, file upload, source filtering, chat interface, graph visualization, evidence cards, report download

---

## ğŸ—ï¸ Architecture

### System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Interface Layer                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Rich CLI           â”‚      Streamlit GUI               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                        â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Application Core                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Ingester   â”‚ VectorStore  â”‚    Tracer                 â”‚
â”‚              â”‚              â”‚                           â”‚
â”‚ â€¢ PDF        â”‚ â€¢ ChromaDB   â”‚ â€¢ LLM Query Analysis      â”‚
â”‚ â€¢ CSV/TXT    â”‚ â€¢ Embeddings â”‚ â€¢ Citation Generation     â”‚
â”‚ â€¢ Excel      â”‚ â€¢ Metadata   â”‚ â€¢ Context Formatting      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Validation & Reporting Layer                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Validator   â”‚  Reporter    â”‚    Graph                  â”‚
â”‚              â”‚              â”‚                           â”‚
â”‚ â€¢ Conflicts  â”‚ â€¢ Markdown   â”‚ â€¢ Entity Extraction       â”‚
â”‚ â€¢ Reliabilityâ”‚ â€¢ Timestamps â”‚ â€¢ Knowledge Graphs        â”‚
â”‚ â€¢ Scoring    â”‚ â€¢ Evidence   â”‚ â€¢ Visualization           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   External Services                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      Ollama API      â”‚      ChromaDB                    â”‚
â”‚   (LLM Inference)    â”‚   (Vector Storage)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Documents   â”‚ (PDF/CSV/TXT/XLSX)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ingester    â”‚ â†’ Chunks (800 chars, 100 overlap)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   Metadata (source, page, timestamp)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VectorStore  â”‚ â†’ Embeddings â†’ ChromaDB
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Query
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ VectorStore  â”‚ â†’ Top-K Retrieval â†’ Chunks
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Tracer     â”‚  â”‚  Validator   â”‚
â”‚ (Analysis)   â”‚  â”‚ (Conflicts)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Reporter   â”‚ â†’ Markdown Report
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

- **`Ingester`**: Document processing and chunking (PDF, CSV, TXT, Excel)
- **`CerebroVectorStore`**: ChromaDB vector database management
- **`CerebroTracer`**: LLM-powered query analysis with citation generation
- **`CerebroValidator`**: Conflict detection and reliability scoring
- **`CerebroReporter`**: Markdown report generation
- **`CerebroGraph`**: Knowledge graph visualization (spaCy NER)
- **`CerebroFormatter`**: Rich CLI formatting

---

## ğŸ”§ Troubleshooting

| Issue | Solution |
|-------|----------|
| Ollama connection error | Run `ollama serve` in separate terminal |
| No models available | Download with `ollama pull llama3` |
| spaCy model missing | Run `python -m spacy download en_core_web_sm` |
| ChromaDB errors | Reset database: `rm -rf data/chroma_db` |
| Import errors | Reinstall: `pip install -r requirements.txt` |
| Memory issues | Reduce `chunk_size` or process in batches |

---

## ğŸš€ Performance Optimization

### For Large Document Collections

1. **Batch Processing**: Process documents in smaller batches
   ```python
   # Process files in batches of 10
   for batch in chunked(files, 10):
       chunks = ingester.ingest_directory(batch)
       vector_db.add_chunks(chunks)
   ```

2. **Optimize Chunking**: Adjust chunk size based on document type
   ```python
   # For technical documents
   ingester = Ingester(chunk_size=1000, chunk_overlap=150)
   
   # For structured data (CSV)
   ingester = Ingester(chunk_size=500, chunk_overlap=50)
   ```

3. **Database Indexing**: ChromaDB automatically indexes, but you can:
   - Use persistent storage (default)
   - Monitor database size with `du -sh data/chroma_db`

4. **Model Selection**: Use smaller models for faster inference
   ```bash
   # Faster but less accurate
   ollama pull phi
   
   # Balanced
   ollama pull llama3
   
   # Slower but more accurate
   ollama pull mistral
   ```

### For Production Deployment

1. **Use Production Ollama**: Deploy Ollama as a service
2. **Database Backup**: Regularly backup `data/chroma_db`
3. **Monitor Resources**: Track CPU, RAM, and disk usage
4. **Caching**: Implement result caching for frequent queries
5. **Load Balancing**: Use multiple Ollama instances for high traffic

---

## ğŸ“Š Use Cases

- **Legal Analysis**: Search case files, identify precedents, detect contradictions
- **Research Review**: Analyze papers, detect conflicting methodologies, extract entities
- **Business Intelligence**: Query financial reports, extract metrics, generate insights
- **Compliance Auditing**: Check policy violations, generate audit trails
- **Knowledge Base**: Internal Q&A system with traceable citations

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m "Add amazing feature"`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open Pull Request

**Code Style**: Follow PEP 8, use type hints, add docstrings.

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

### Core Technologies
- **[Ollama](https://ollama.ai/)** - Local LLM inference engine
- **[ChromaDB](https://www.trychroma.com/)** - Open-source vector database
- **[Streamlit](https://streamlit.io/)** - Rapid web app development
- **[Rich](https://github.com/Textualize/rich)** - Beautiful terminal formatting
- **[spaCy](https://spacy.io/)** - Natural language processing

### Inspiration
Built with â¤ï¸ for professionals who need transparent, traceable, and auditable AI-powered document analysis.

---

## ğŸ“ Support & Contact

- **Issues**: [GitHub Issues](https://github.com/yourusername/LLM-CerebroScope/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/LLM-CerebroScope/discussions)
- **Email**: support@example.com (update with your contact)

---

<div align="center">

**Made with ğŸ” by the CerebroScope Team**

*Empowering transparent AI-powered document intelligence*

[â¬† Back to Top](#-llm-cerebroscope)

</div>

